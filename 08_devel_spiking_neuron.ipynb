{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Open Letter to Pi,\n",
    "\n",
    "Hey Pi,\n",
    "\n",
    "Thank you for the discussion yesterday evening, about the stateful neurons and the architecture for image recognition,\n",
    "\n",
    "For the purpose of implementing it, I still think that Image recognition from \"videos of image\" where the image is presented to the NN \"over a longer time\" is the right first test for this concept,\n",
    "\n",
    "and if it works, you may find that for language modelling, each token needs also to be presented to the NN \"for a while\" before it gets processed by these stateful spiking neurons -- in other words, kind of like with RRNs, you have to allow for the recurrent state to make several rounds \n",
    "which brings me to 3 points about this idea:\n",
    "The spiking stochastic nature means that it may take a long time for the network to \"learn from zero\" about something. Think about it -- the \"target neuron\" can only fire if all the right connections have fired, and only then it gets positive feedback. In other words, this network suffers from not having gradients but only testing space that was randomly explored. Which brings me to the next point...\n",
    "Reinforcing good connections may be a good idea -- in that it works to densify the acquired knowledge. At first, the NN will only weakly guess the right answer \"using nearly the entire brain\", but then, the connections that are good will prune out all the connections that are not needed for this bit of knowledge, leaving them available to act on other knowledge. This may be an excellent feature of this approach. However,\n",
    "This also means that when \"in training\", there must be some non zero base firing rate for all the neurons irrespectively if they detect a feature or not. If this doesn't happen, then they will never learn. This also implies that there needs to be a fairly high tolerance on ocassionally -- let's say, 20% or even down to 1% -- providing a bad label at the end. This is OK during training but may or may not be acceptable in production for any given application. This is certainly not what your typical customer expects from a computer or any machine for that matter.  UPDATE: OR, one could simply do the production time inference twice during production. Once with \"non-training mode\" and once with \"training/exploration mode\" where higher base spike rate is used, and then used for training step.\n",
    "All this brings in amazing parallels with how real living things learn!!! isn't that fascinating!!! Just think about all the cases when you have seen or heard about some new fact, and your brain has replayed it on the inside over and over again, only to get a \"click\" moment many days later. This is where the \"correct\" neuron randomly fires enough many times to build it's connection, and suddenly you get this warm feeling of grok.\n",
    "I understand your desire to do online learning, where a NN in production gets trained while in production. Just thinking about it tho, such a feat is very much possible to do with regular Artificial NNs that also benefit from the gradient. THe process would be to simply raise the temperature of the final token selector, select \"creative tokens\" and then use some kind of slow-thinking process to check it's quality; then back-propagate to reinforce the path that created it, and contrast-suppress the paths that contributed to other solutions. Since you can always recompute the gradients \"in the post\", this can be either an online or offline process.  To conclude, the gradients make the learning way, way faster, but maybe a bit too fast\n",
    "\n",
    "In general, I think it's a good idea to try and implement, starting with basic python, with following features as discussed:\n",
    "\n",
    "* positive integer states only, and integers saturate at 15.\n",
    "* Firing is stochastic, and PRNG driven;  e.g. https://en.wikipedia.org/wiki/Well_equidistributed_long-period_linear http://lomont.org/papers/2008/Lomont_PRNG_2008.pdf\n",
    "* The \"excitement state\" accumulator is also integer, but can be decayed slowly using the \"decay by chance\" technique\n",
    "when in \"training\" state, the base \"untrained\" firing rate must be significantly above zero to produce enough chance for the connections to form. One can call this a \"young network\"\n",
    "* The connection list for each neuron can be recomputed on the fly using a very simple PRNG seeded from the neuron's coordinate. Such re computation is effectively cheaper than storing a look-up table. Again -- treat memory access as expensive, and computation as cheap.\n",
    "* For small networks, you may be able to fit entire thing, or at least the neuron state in just registers (mega fast). There is exactly 64KB of registers per SM. Each byte can hold two states for us. There are either 16 or 32 cuda cores per SM, depending on the version of the GPU, so for example, for A100, you can count on there being 216SMs, for a total of 13824kB of unique registers, or  28'311'552 (28M) unique states -- which is quite a lot really.\n",
    "* The L1 cache and L2 cache is only good for situations where the data can be broadcast to many cores, or in other words \"write once read many\".  For our application, it will be needed to broadcast the firing of any given neuron. Let's say that we can encode that the neuron has fired at all by just one bit. For A100, The shared memory size is up to 164kB per SM if we disable the self-managed L1 completely, and again, there are 216SMs, meaning 290'193'408 bits (290M !) neuron firings can be expressed at any time. I think you will agree that this is quite a good number!\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e21db43c1d339d55"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import jax.numpy as jnp"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T20:53:05.259278769Z",
     "start_time": "2024-02-04T20:53:04.938719069Z"
    }
   },
   "id": "d5fa492132b07d98",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "z=jnp.zeros(shape=1, dtype=jnp.int32)+1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-04T20:53:29.048325678Z",
     "start_time": "2024-02-04T20:53:29.017054396Z"
    }
   },
   "id": "8ab94de41151b199",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "class SpikingNeuronGroup:\n",
    "    def __init__(self, neuronsInGroup = 4, baseFiringRate = 1e-3):\n",
    "        self.GroupActivationPotential : jax.numpy.array = jnp.zeros(shape=0, dtype=jnp.int32)\n",
    "        self.ActivationStates : jax.numpy.array = jnp.zeros(shape=(neuronsInGroup,), dtype=jnp.int32)\n",
    "        self.SourceNeuronList : []\n",
    "        self.PerSourceNeuronWeights : jax.numpy.array = jnp.zeros(shape=(neuronsInGroup,), dtype=jnp.int32)\n",
    "        self.outputState : jax.numpy.array = jnp.zeros(shape=(neuronsInGroup,), dtype=jnp.int32)\n",
    "        self.baseFiringRate = baseFiringRate\n",
    "        \n",
    "    def cycle(self):\n",
    "        self.outputState = jnp.zeros(shape=(neuronsInGroup,), dtype=jnp.int32)\n",
    "        # group activation potential\n",
    "        groupActivationPotential = 0\n",
    "        for i in range(len(SourceNeurons)):\n",
    "            activateionPotential \n",
    "            if self.ActivationStates[i] > 0:\n",
    "                self.outputState += self.PerSourceNeuronWeights[i]\n",
    "        self.ActivationStates = jnp.maximum(jnp.zeros(shape=(neuronsInGroup,), dtype=jnp.int32), self.ActivationStates - 1)\n",
    "        self.ActivationStates += jnp.random.randint(0, 2, shape=(neuronsInGroup,), dtype=jnp.int32)\n",
    "        return self.outputState\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
